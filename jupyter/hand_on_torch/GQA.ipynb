{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd0eb01",
   "metadata": {},
   "source": [
    "# GQA的手撕\n",
    "话不多说，直接开始\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0516a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每日一导\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a852b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_heads,n_kv_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        assert n_heads % n_kv_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.head_dim = d_model //n_heads\n",
    "        # 计算每个分组的大小\n",
    "        self.group_size = n_heads // n_kv_heads \n",
    "\n",
    "        # 初始化4个Linear\n",
    "        self.w_q = nn.Linear(d_model,n_heads * self.head_dim)\n",
    "        self.w_k = nn.Linear(d_model,n_kv_heads*self.head_dim)\n",
    "        self.w_v = nn.Linear(d_model,n_kv_heads*self.head_dim)\n",
    "        self.w_o = nn.Linear(n_heads * self.head_dim, d_model)\n",
    "    \n",
    "    def repeat_kv(self, x, n_rep):\n",
    "        if n_rep == 1:\n",
    "            return x\n",
    "        batch_size, n_kv_heads, seq_len, head_dim = x.shape\n",
    "        x = x[:, :, None, :, :]  # (B, n_kv_heads, 1, T, D)\n",
    "        x = x.expand(batch_size, n_kv_heads, n_rep, seq_len, head_dim)\n",
    "        return x.reshape(batch_size, n_kv_heads * n_rep, seq_len, head_dim)\n",
    "        \n",
    "    def forward(self,x,mask=None):\n",
    "        batch_size,seq_len,_ =x.shape\n",
    "        # 1 projection\n",
    "        q = self.w_q(x).view(batch_size,seq_len,self.n_heads,self.head_dim).transpose(1,2)\n",
    "        k = self.w_k(x).view(batch_size,seq_len,self.n_kv_heads,self.head_dim).transpose(1,2)\n",
    "        v = self.w_v(x).view(batch_size,seq_len,self.n_kv_heads,self.head_dim).transpose(1,2)\n",
    "\n",
    "        # 2 lift dim\n",
    "        k = self.repeat_kv(k,self.group_size)\n",
    "        v = self.repeat_kv(v,self.group_size)\n",
    "\n",
    "        # 3 compute attn scroes\n",
    "        attn_scores = torch.matmul(q,k.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # 4 check if mask\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_score.masked_fill(mask==0,-1e9)\n",
    "\n",
    "        # 5 Normalization\n",
    "        attn_prob = F.softmax(attn_scores,dim=-1)\n",
    "\n",
    "        # 6 weighed sum\n",
    "        output = torch.matmul(attn_prob,v)\n",
    "\n",
    "        # 7 concat multi heads\n",
    "        output =output.transpose(1,2).contiguous().view(batch_size,seq_len,-1)\n",
    "\n",
    "        # 8 last linear\n",
    "        return self.w_o(output)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51fa48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupedQueryAttention(\n",
      "  (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (w_k): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (w_v): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (w_o): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "torch.Size([1, 4, 128])\n",
      "torch.Size([1, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "model = GroupedQueryAttention(d_model=128,n_heads=8,n_kv_heads=2)\n",
    "x = torch.randn(1,4,128)\n",
    "_ = model(x)\n",
    "print(model)\n",
    "print(x.shape)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac248a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
